{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"15.12.LørdagWorking_Boxes.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"b_SwNLV4d0wF","outputId":"d636cf61-3333-4583-fcee-6fff0cc64792"},"source":["# Rynkeby Thor\n","# Installer pakker\n","import os\n","import numpy as np\n","import torch\n","import torch.utils.data\n","from PIL import Image\n","import xml.etree.ElementTree as ET\n","from google.colab import drive\n","from torchvision import transforms\n","\n","import torchvision\n","from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n","\n","drive.mount('/content/gdrive', force_remount=True)\n","os.chdir('/content/gdrive/My Drive/Projekt20TeamRynkeby')"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive\n"]}]},{"cell_type":"code","metadata":{"id":"Bz3V_vXqbNPL"},"source":["### Rynkeby \n","# Class\n","import os, psutil\n","process = psutil.Process(os.getpid())\n","\n","\n","class PennFudanDataset(torch.utils.data.Dataset):\n","    def __init__(self, root, transforms=None):\n","        self.root = root\n","        self.transforms = transforms\n","        # load all image files, sorting them to\n","        # ensure that they are aligned\n","        n = 1500\n","        start = 1\n","        self.imgs = list(sorted(os.listdir(\"Filtered_frames2\")))#[start:start+n]#\n","        self.allxml = sorted(os.listdir('framesXML2'))#[start:start+n]#\n","        self.allFilterFrames = sorted(os.listdir('Filtered_frames2'))#[start:start+n]#\n","\n","    def __getitem__(self, idx):\n","\n","        #framenumber = f\"{idx:06}\"\n","        xml = self.allxml[idx]\n","        \n","        #tree = ET.parse('project_20_data/video1/framesXML/frame_{}.xml'.format(framenumber))\n","        tree = ET.parse('framesXML2/{}'.format(xml))      \n","        root = tree.getroot()\n","\n","        labels = [0] * 10\n","        imgString = self.allFilterFrames[idx]\n","        #print(imgString)\n","        objects = 0\n","        var=0\n","        # print all the objects\n","        for obj in root.iter('object'):\n","          name = obj.find('name').text\n","\n","          if name == 'beer':\n","            var = 1\n","          elif name == 'cola':\n","            var = 2\n","          #else:\n","          #  var = 420\n","          #print('var: {}'.format(var))\n","          # print('idx: {}'.format(idx))\n","          # print('imgString: {}\\n'.format(imgString))    \n","          process = psutil.Process(os.getpid())\n","          #print('in bytes: {}\\n'.format(process.memory_info().rss))   \n","\n","          labels[objects] = var\n","          objects = objects + 1\n","          \n","          #labels.append(var)\n","          #print((labels))\n","        boxes = []\n","        area = []\n","        #print(labels)\n","        if all(v == 0 for v in labels):\n","          ghjksd=0\n","          #print('No objects in image')\n","        else:\n","        #if labels: #it calculates area if there are Labels\n","          # Find and save all coordinates of box for each object\n","          for features in root.iter('bndbox'):\n","\n","            xmin = float(features.find('xmin').text)\n","            xmax = float(features.find('xmax').text)\n","            ymin = float(features.find('ymin').text)\n","            ymax = float(features.find('ymax').text)\n","            boxes.append([xmin, ymin, xmax, ymax])\n","\n","          boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","          area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n","          area = torch.as_tensor(area,dtype=torch.float32)\n","          \n","        # Test if no boxes\n","        if objects == 0:\n","          boxes = torch.empty(0,4)\n","          area = torch.tensor([0])\n","\n","        # Make sure the boxes is converted even if there is no objects\n","        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n","        labels = list(filter((0).__ne__, labels))\n","        labels = torch.as_tensor(labels, dtype=torch.int64)\n","\n","        img_path = 'Filtered_frames2/{}'.format(imgString)\n","        img = Image.open(img_path).convert('RGB')\n","        #display(img)\n","        \n","        target = {}\n","        target[\"boxes\"] = boxes\n","        target[\"labels\"] = labels\n","        target[\"image_id\"] = torch.as_tensor(idx, dtype=torch.int64)\n","        target[\"area\"] = area\n","        target[\"iscrowd\"] = torch.ones(objects, dtype=torch.uint8)\n","\n","        if self.transforms is not None:      \n","            img, target = self.transforms(img, target)\n","        return img, target\n","\n","    def __len__(self):\n","        return len(self.imgs)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"J6f3ZOTJ4Km9"},"source":["That's all for the dataset. Let's see how the outputs are structured for this dataset"]},{"cell_type":"code","metadata":{"id":"ZEARO4B_ye0s"},"source":["#dataset = PennFudanDataset('')\n","\n","#Undersøg data\n","# step = int(np.rint(len(dataset)*0.05))\n","# for i in range(0,len(dataset)-1,step):\n","#    print(i)\n","#    display(dataset[i][0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lWOhcsir9Ahx"},"source":["So we can see that by default, the dataset returns a `PIL.Image` and a dictionary\n","containing several fields, including `boxes`, `labels` and `masks`."]},{"cell_type":"markdown","metadata":{"id":"-WXLwePV5ieP"},"source":["That's it, this will make model be ready to be trained and evaluated on our custom dataset.\n","\n","## Training and evaluation functions\n","\n","In `references/detection/,` we have a number of helper functions to simplify training and evaluating detection models.\n","Here, we will use `references/detection/engine.py`, `references/detection/utils.py` and `references/detection/transforms.py`.\n","\n","Let's copy those files (and their dependencies) in here so that they are available in the notebook"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":53},"id":"UYDb7PBw55b-","outputId":"3fb02781-2b83-4c8e-edc8-8b95efcc946b"},"source":["'''\n","%%shell\n","\n","# Download TorchVision repo to use some files from\n","# references/detection\n","git clone https://github.com/pytorch/vision.git\n","cd vision\n","git checkout v0.8.2\n","\n","\n","cp references/detection/utils.py ../\n","cp references/detection/transforms.py ../\n","cp references/detection/coco_eval.py ../\n","cp references/detection/engine.py ../\n","cp references/detection/coco_utils.py ../\n","'''"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'\\n%%shell\\n\\n# Download TorchVision repo to use some files from\\n# references/detection\\ngit clone https://github.com/pytorch/vision.git\\ncd vision\\ngit checkout v0.8.2\\n\\n\\ncp references/detection/utils.py ../\\ncp references/detection/transforms.py ../\\ncp references/detection/coco_eval.py ../\\ncp references/detection/engine.py ../\\ncp references/detection/coco_utils.py ../\\n'"]},"metadata":{},"execution_count":38}]},{"cell_type":"markdown","metadata":{"id":"2u9e_pdv54nG"},"source":["\n","\n","Let's write some helper functions for data augmentation / transformation, which leverages the functions in `refereces/detection` that we have just copied:\n"]},{"cell_type":"code","metadata":{"id":"l79ivkwKy357"},"source":["# Genstart kørsels tid, hvis den ikke virker\n","from engine import train_one_epoch, evaluate\n","import utils\n","import transforms as T\n","\n","def get_transform(train):\n","    transforms = []\n","    # converts the image from a PIL image to a PyTorch Tensor\n","    transforms.append(T.ToTensor())\n","    \n","    if train:\n","        # during training, randomly flip the training images\n","        # and ground-truth for data augmentation\n","        transforms.append(T.RandomHorizontalFlip(0.5))\n","    return T.Compose(transforms)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ovjC81FZfWat"},"source":["#### Testing forward() method \n","\n","Before iterating over the dataset, it’s good to see what the model expects during training and inference time on sample data.\n"]},{"cell_type":"markdown","metadata":{"id":"FzCLqiZk-sjf"},"source":["#### Note that we do not need to add a mean/std normalization nor image rescaling in the data transforms, as those are handled internally by the Mask R-CNN model."]},{"cell_type":"markdown","metadata":{"id":"3YFJGJxk6XEs"},"source":["### Putting everything together\n","\n","We now have the dataset class, the models and the data transforms. Let's instantiate them"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a5dGaIezze3y","outputId":"0e66ba24-ca45-44d0-85b9-6ce38af06c86"},"source":["# use our dataset and defined transformations\n","dataset = PennFudanDataset('',get_transform(train=True))\n","dataset_test = PennFudanDataset('',get_transform(train=False))\n","\n","# split the dataset in train and test set\n","# torch.manual_seed(1)\n","\n","indices= torch.arange(0,0+len(dataset)).tolist() # Denne linje er ny\n","\n","dataset = torch.utils.data.Subset(dataset, indices[:-300])\n","\n","dataset_test = torch.utils.data.Subset(dataset_test, indices)#[-200:]\n","\n","n = 4\n","m = 4\n","# define training and validation data loaders\n","data_loader = torch.utils.data.DataLoader(\n","    dataset, batch_size=n, shuffle=True, num_workers=2,\n","    collate_fn=utils.collate_fn)\n","\n","data_loader_test = torch.utils.data.DataLoader(\n","    dataset_test, batch_size=m, shuffle=False, num_workers=2,\n","    collate_fn=utils.collate_fn)\n","\n","TrainTal = len(dataset)/n\n","TestTal = len(dataset_test)/m\n","print('TrainTal er :{}\\nTestTal  er :{}'.format(TrainTal,TestTal))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["TrainTal er :300.0\n","TestTal  er :375.0\n"]}]},{"cell_type":"markdown","metadata":{"id":"L5yvZUprj4ZN"},"source":["Now let's instantiate the model and the optimizer"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zoenkCj18C4h","outputId":"399b67d1-e49f-4ef2-cf3f-ff3de83029e7"},"source":["device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n","\n","# our dataset has two classes only - background and person\n","# This dataset contains 3 classes: Beer (1), Coke (2) and background (0)\n","num_classes = 3\n","\n","# Get network\n","model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n","# move model to the right device\n","model.to(device)\n","\n","in_features = model.roi_heads.box_predictor.cls_score.in_features\n","# replace the pre-trained head with a new one\n","model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","\n","# move model to the right device\n","model.to(device)\n","\n","# construct an optimizer\n","params = [p for p in model.parameters() if p.requires_grad]\n","optimizer = torch.optim.SGD(params, lr=0.005,\n","                            momentum=0.9, weight_decay=0.0005)\n","\n","# and a learning rate scheduler which decreases the learning rate by\n","# 10x every 3 epochs\n","lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer,\n","                                               step_size=3,\n","                                               gamma=0.1)\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","metadata":{"id":"XAd56lt4kDxc"},"source":["And now let's train the model for 10 epochs, evaluating at the end of every epoch."]},{"cell_type":"code","metadata":{"id":"at-h4OWK0aoc"},"source":["# # let's train it for some epochs\n","# from torch.optim.lr_scheduler import StepLR\n","# num_epochs = 3 # ctr shift 7\n","\n","# for epoch in range(num_epochs):\n","#     # train for one epoch, printing every 10 iterations\n","#     train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq=25)\n","#     # update the learning rate\n","#     lr_scheduler.step()\n","#     # evaluate on the test dataset\n","#     evaluate(model, data_loader_test, device=device)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vMMMIikNlvFU"},"source":["model = torch.load('/content/gdrive/My Drive/Projekt20TeamRynkeby/model:LR=0.005,MOM=0.9, WD=0.0005,SS=3,G=0.1,EPOCHS=3.pt')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z6mYGFLxkO8F"},"source":["Now that training has finished, let's have a look at what it actually predicts in a test image"]},{"cell_type":"code","metadata":{"id":"prqfjYa-TXV8"},"source":["# import the necessary packages\n","from scipy.spatial import distance as dist\n","from collections import OrderedDict\n","import numpy as np\n","class centroidTyuracker():\n","\tdef __init__(self, maxDisappeared=5):\n","\t\t# initialize the next unique object ID along with two ordered\n","\t\t# dictionaries used to keep track of mapping a given object\n","\t\t# ID to its centroid and number of consecutive frames it has\n","\t\t# been marked as \"disappeared\", respectively\n","\t\tself.nextObjectID = 0\n","\t\tself.objects = OrderedDict()\n","\t\tself.disappeared = OrderedDict()\n","\t\tself.ClassO = 1#np.zeros(100)\n","\t\t# store the number of maximum consecutive frames a given\n","\t\t# object is allowed to be marked as \"disappeared\" until we\n","\t\t# need to deregister the object from tracking\n","\t\tself.maxDisappeared = maxDisappeared\n","\t\tprint('__init__')\n","\t\n","\n","         \n","\tdef register(self, centroid):\n","\t\t#print('Register')\n","\t\t# when registering an object we use the next available object\n","\t\t# ID to store the centroid\n","\t\tself.objects[self.nextObjectID] = centroid\n","\t\tself.disappeared[self.nextObjectID] = 0\n","\t\n","\t\tself.nextObjectID += 1\n","\n","\tdef deregister(self, objectID):\n","\t\t#print('In deregister')\n","\t\t# to deregister an object ID we delete the object ID from\n","\t\t# both of our respective dictionaries\n","\t\tdel self.objects[objectID]\n","\t\tdel self.disappeared[objectID]\n","\n","\tdef update(self, rects): #,classI\n","\t\t# check to see if the list of input bounding box rectangles\n","\t\t# is empty\n","\t\tif len(rects) == 0:\n","\t\t\t# loop over any existing tracked objects and mark them\n","\t\t\t# as disappeared\n","\t\t\tfor objectID in list(self.disappeared.keys()):\n","\t\t\t\tself.disappeared[objectID] += 1\n","\t\t\t\t# if we have reached a maximum number of consecutive\n","\t\t\t\t# frames where a given object has been marked as\n","\t\t\t\t# missing, deregister it\n","\t\t\t\tif self.disappeared[objectID] > self.maxDisappeared:\n","\t\t\t\t\t#print('rect =0')\n","\t\t\t\t\tself.deregister(objectID)\n","\t\t\t# return early as there are no centroids or tracking info\n","\t\t\t# to update\n","\t\t\treturn self.objects\n","\t\t# initialize an array of input centroids for the current frame\n","\t\tinputCentroids = np.zeros((len(rects), 2), dtype=\"int\")\n","\t\t# loop over the bounding box rectangles\n","\t\tfor (i, (startX, startY, endX, endY)) in enumerate(rects):\n","\t\t\t# use the bounding box coordinates to derive the centroid\n","\t\t\tcX = int((startX + endX) / 2.0)\n","\t\t\tcY = int((startY + endY) / 2.0)\n","\t\t\tinputCentroids[i] = (cX, cY)\n","\t\t# if we are currently not tracking any objects take the input\n","\t\t# centroids and register each of them\n","\n","\t\tif len(self.objects) == 0:\n","\t\t\tfor i in range(0, len(inputCentroids)):\n","\t\t\t\tself.register(inputCentroids[i])\n","\t\t# otherwise, are are currently tracking objects so we need to\n","\t\t# try to match the input centroids to existing object\n","\t\t# centroids\n","\t\telse:\n","\t\t\t# grab the set of object IDs and corresponding centroids\n","\t\t\tobjectIDs = list(self.objects.keys())\n","\t\t\tobjectCentroids = list(self.objects.values())\n","\t\t\t# compute the distance between each pair of object\n","\t\t\t# centroids and input centroids, respectively -- our\n","\t\t\t# goal will be to match an input centroid to an existing\n","\t\t\t# object centroid\n","\t\t\tD = dist.cdist(np.array(objectCentroids), inputCentroids)\n","\t\t\t# in order to perform this matching we must (1) find the\n","\t\t\t# smallest value in each row and then (2) sort the row\n","\t\t\t# indexes based on their minimum values so that the row\n","\t\t\t# with the smallest value is at the *front* of the index\n","\t\t\t# list\n","\t\t\t#rows = D.min(axis=1).argsort()\n","\t\t\t# next, we perform a similar process on the columns by\n","\t\t\t# finding the smallest value in each column and then\n","\t\t\t# sorting using the previously computed row index list\n","\t\t\t#cols = D.argmin(axis=1)[rows]\n","\t\t\tcols = D.min(axis=0).argsort()\n","\t\t\trows = D.argmin(axis=0)[cols]\n","\t\t\t# in order to determine if we need to update, register,\n","\t\t\t# or deregister an object we need to keep track of which\n","\t\t\t# of the rows and column indexes we have already examined\n","\t\t\tusedRows = set()\n","\t\t\tusedCols = set()\n","\t\t\t# loop over the combination of the (row, column) index\n","\t\t\t# tuples\n","\t\t\tfor (row, col) in zip(rows, cols):\n","\t\t\t\t# if we have already examined either the row or\n","\t\t\t\t# column value before, ignore it\n","\t\t\t\t# val\n","\t\t\t\tif row in usedRows or col in usedCols:\n","\t\t\t\t\tcontinue\n","\t\t\t\t# otherwise, grab the object ID for the current row,\n","\t\t\t\t# set its new centroid, and reset the disappeared\n","\t\t\t\t# counter\n","\t\t\t\tobjectID = objectIDs[row]\n","\t\t\t\tself.objects[objectID] = inputCentroids[col]\n","\t\t\t\tself.disappeared[objectID] = 0\n","\t\t\t\t# indicate that we have examined each of the row and\n","\t\t\t\t# column indexes, respectively\n","\t\t\t\tusedRows.add(row)\n","\t\t\t\tusedCols.add(col)\n","\t\t\t# compute both the row and column index we have NOT yet\n","\t\t\t# examined\n","\t\t\tunusedRows = set(range(0, D.shape[0])).difference(usedRows)\n","\t\t\tunusedCols = set(range(0, D.shape[1])).difference(usedCols)\n","\t\t\t# in the event that the number of object centroids is\n","\t\t\t# equal or greater than the number of input centroids\n","\t\t\t# we need to check and see if some of these objects have\n","\t\t\t# potentially disappeared\n","\t\t\tif D.shape[0] >= D.shape[1]:\n","\t\t\t\t# loop over the unused row indexes\n","\t\t\t\tfor row in unusedRows:\n","\t\t\t\t\t# grab the object ID for the corresponding row\n","\t\t\t\t\t# index and increment the disappeared counter\n","\t\t\t\t\tobjectID = objectIDs[row]\n","\t\t\t\t\tself.disappeared[objectID] += 1\n","\t\t\t\t\t# check to see if the number of consecutive\n","\t\t\t\t\t# frames the object has been marked \"disappeared\"\n","\t\t\t\t\t# for warrants deregistering the object\n","\t\t\t\t\tif self.disappeared[objectID] > self.maxDisappeared:\n","\t\t\t\t\t\t#print('rect s0')\n","\t\t\t\t\t\tself.deregister(objectID)\n","\t\t\t# otherwise, if the number of input centroids is greater\n","\t\t\t# than the number of existing object centroids we need to\n","\t\t\t# register each new input centroid as a trackable object\n","\t\t\telse:\n","\t\t\t\tfor col in unusedCols:\n","\t\t\t\t\tself.register(inputCentroids[col])\n","\t\t# return the set of trackable objects\n","\t\tself.ClassO=1\n","\n","\t\treturn self.objects\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0g1C3SDxtGvG"},"source":["# import the necessary packages\n","from collections import namedtuple\n","import numpy as np\n","import cv2\n","# define the `Detection` object\n","# (startX, startY, endX, endY)\n","def bb_intersection_over_union(boxA, boxB):\n","\n","\t# determine the (x, y)-coordinates of the intersection rectangle\n","\txA = max(boxA[0], boxB[0])\n","\tyA = max(boxA[1], boxB[1])\n","\txB = min(boxA[2], boxB[2])\n","\tyB = min(boxA[3], boxB[3])\n","\t#print(yB)\n","\tsubmerging = 0\n","\tif (boxA[0] > boxB[0]) and (boxA[1] > boxB[1]) and (boxA[2] < boxB[2]) and (boxA[3] < boxB[3]):\n","\t\tsubmerging = 1\n","\tif (boxA[0] < boxB[0]) and (boxA[1] < boxB[1]) and (boxA[2] > boxB[2]) and (boxA[3] > boxB[3]):\n","\t\tsubmerging = 1\n","\n","\t# compute the area of intersection rectangle\n","\tinterArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n","\t# compute the area of both the prediction and ground-truth\n","\t# rectangles\n","\tboxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n","\tboxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n","\t# compute the intersection over union by taking the intersection\n","\t# area and dividing it by the sum of prediction + ground-truth\n","\t# areas - the interesection area\n","\tiou = interArea / float(boxAArea + boxBArea - interArea)\n","\n","\t# return the intersection over union value\n","\treturn iou, submerging\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ueuSZ0YZFhAj"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"m5kyKxDMELga"},"source":["def UniqueBox(predictions):\n","  #Determining unike boxes\n","  using = np.ones(len(predictions))\n","  value=0\n","  for idx1 in range(len(predictions)):\n","\n","    for idx2 in range(len(predictions)):\n","\n","      uiden = bb_intersection_over_union(predictions[idx1], predictions[idx2])\n","\n","      if ((uiden[0]>0.3) or (uiden[1] == 1)) and (idx1 < idx2):\n","        using[idx2] = value\n","        #value = value+1\n","  for i in range(len(predictions)):\n","    if (using[i] == 1):\n","      using[i] = i+1\n","  \n","  return using"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"f2i0pfnlap3s"},"source":["def totuple(a):\n","    try:\n","        return tuple(totuple(i) for i in a)\n","    except TypeError:\n","        return a"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kuY3Eq6Kfb-i","outputId":"d3c270c6-6987-41f0-bb90-4ae4301ab9ad"},"source":["len(dataset_test)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["1500"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","metadata":{"id":"nw6PXRQMb6fs"},"source":["from imutils.video import VideoStream\n","import argparse\n","import imutils\n","import time\n","import matplotlib.pyplot as plt\n","import matplotlib.patches as patches\n","\n","\n","ct = centroidTyuracker()\n","\n","for PicNum in range (1200):\n","  # pick one image from the test set\n","  test_image = PicNum+776\n","  img, _ = dataset_test[test_image]\n","  # put the model in evaluation mode\n","  model.eval()\n","  with torch.no_grad():\n","      prediction = model([img.to(device)])\n","\n","  # Make threshold for scores\n","  scores = (prediction[0]['scores'])\n","  threshhold = (scores > 0.95).nonzero(as_tuple=True)[0]# Counter. Hver gang score > T, tilføjes nyt tal til rækken [0,1,...N]\n","\n","  # Create figure and axes\n","  fig, ax = plt.subplots()\n","\n","  # Display the image\n","  Image.fromarray(prediction[0]['boxes'].mul(255).byte().cpu().numpy())\n","  tensor_image = Image.fromarray(img.mul(255).permute(1, 2, 0).byte().numpy())\n","  ax.imshow(tensor_image)\n","\n","  rects = []\n","  Unique = UniqueBox(prediction[0]['boxes']) #The unique idendified boxes\n","  NrOfObjects=len((np.where(Unique != 0))[0])# Amount of unique objects\n","  #print('Amount of Unique objects: {}'.format(NrOfObjects))\n","  if (len(Unique)!=0):\n","\n","    # Gennemgår alle boxes med score over threshhold\n","    edgecolor=[]\n","    CanClass = []\n","    for i in range(len(threshhold)):\n","      \n","      if (Unique[i]!=0): #only use the unique boxes:\n","        # Box\n","        B = prediction[0]['boxes'][threshhold[i]].cpu().squeeze().numpy()\n","        \n","        # Hvis det er den første box:\n","        if i == 0:\n","          rects=np.append([B],[B], axis=0)\n","        \n","        else:\n","          rects=np.append(rects,[B], axis=0)\n","        \n","        # Sørger for at boxen for i == 0 ikke optræder 2 gange\n","        rects=rects[0:i+1]\n","        rects = totuple(rects)\n","        # Create a Rectangle patch (start coordinate), width, hight\n","\n","\n","        # Add label color\n","        if prediction[0]['labels'][threshhold[i]].cpu().squeeze().numpy() == 1:\n","          edgecolor.append('lime')\n","          CanClass.append(1)\n","\n","        if prediction[0]['labels'][threshhold[i]].cpu().squeeze().numpy() == 2:\n","          edgecolor.append('red')\n","          CanClass.append(2)\n","\n","        box = patches.Rectangle((B[0], B[1]), B[2]-B[0], B[3]-B[1], linewidth=1, edgecolor=edgecolor[i], facecolor='none')\n","\n","        # Add the patch to the Axes\n","        ax.add_patch(box) #makebox\n","        \n","        # Add score to the boxes\n","        score = round(prediction[0]['scores'][threshhold[i]].cpu().squeeze().numpy()*100,2)\n","\n","        rx, ry = box.get_xy()\n","        cx = rx + box.get_width()\n","        cy = ry + box.get_height()\n","        ax.annotate(score, (10+cx, cy), color=edgecolor[i], weight='bold', fontsize=10) #Score%\n","\n","\n","    objects = ct.update(rects)\n","\n","\n","\n","\n","\n","\n","    #if objects is not None and objects != OrderedDict() and type(objects) is not OrderedDict:\n","    centroid = []\n","    objectID =[]\n","    for (objID, cend) in objects.items():\n","      centroid.append(cend)\n","      objectID.append(objID)\n","\n","\n","\n","\n","\n","    match=[]\n","    diff=0\n","    for j in range(len(rects)):\n","      for u in range(len(objects)):\n","\n","        y=0\n","        factorx=(rects[0][2]-rects[0][0])*0.2\n","        factory=(rects[0][3]-rects[0][1])*0.2\n","        re=[rects[j][0]+factorx,rects[j][1]+factory,rects[j][2]-factorx,rects[j][3]-factory]\n","\n","        box=[centroid[u][0],centroid[u][1],centroid[u][0],centroid[u][1]]\n","        y=bb_intersection_over_union(box, re)[1]\n","        if (y==1):\n","          match.append(u)\n","\n","    for i in range(len(match)):\n","\n","        #Centroid\n","      if (CanClass[i]==1):\n","        obj = 'Beer'\n","        color = 'lime'\n","      if (CanClass[i]==2):\n","        obj = 'Coke'\n","        color = 'red'\n","\n","      plt.scatter(centroid[match[i]][0], centroid[match[i]][1],color=color)\n","      ax.annotate('{} {}'.format(obj,objectID[match[i]]+1), (centroid[match[i]][0]+50, centroid[match[i]][1]), color=color, weight='bold', fontsize=10) #Object og Nr\n","\n","    plt.show()\n","  # Show images with boxes\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HiQg_fLcPQt0","outputId":"449e285c-23ab-4e4c-fd0d-448f4aa21573"},"source":["objects.items()\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["odict_items([(3, array([169,  57])), (6, array([239, 335]))])"]},"metadata":{},"execution_count":57}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-6ar77FbNX7z","outputId":"1a2eafae-ca34-4b25-9f50-615a3035a6da"},"source":["centroid[match[i]]\n"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([411, 326])"]},"metadata":{},"execution_count":51}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0GhzvShTbsu","outputId":"9e8ee67f-5b5f-4187-b301-1bb1012823a3"},"source":["objectIDD=[]\n","for (objectID, cend) in objects.items():\n","  objectIDD.append((objectID))\n","print(objectIDD)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[0, 1, 2]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mTLapQKKORWi","outputId":"acf8182e-644b-49d1-836f-c55ab38f3cd7"},"source":["objects is not None and objects != OrderedDict() and type(objects) is not OrderedDict"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["False"]},"metadata":{},"execution_count":53}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WyqFvwFETjS9","outputId":"e60b0529-7968-465c-aec8-eab94783d001"},"source":["for (objectID, centroid) in objects.items():\n","  print(centroid)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[227 317]\n","[334 127]\n","[411 326]\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a_dKs6TCFKy4","outputId":"a2af16cc-19a8-45b6-a9fb-775cce80245d"},"source":["print('rects is:            {}'.format(rects))\n","print('Unique is:           {}'.format(Unique))\n","print('NrObje is:           {}'.format(NrOfObjects))\n","print('Box is:              {}'.format(B))\n","print('Len(T) is:           {}'.format(len(threshhold)))\n","\n","#print('Predicted label is:  {}'.format(prediction[0]['labels'][threshhold[i]].cpu().squeeze().numpy()))\n","\n","\n","print('Predicted Boxes are: {}'.format(prediction[0]['boxes'][threshhold[i]].cpu().squeeze().numpy()))\n","print('Type of boxes are  : {}'.format(type(prediction[0]['boxes'][threshhold[i]].cpu().squeeze().numpy())))\n","print('Len of unique is:    {}'.format(len(Unique)))\n","# print(prediction[0])\n","# print(prediction)\n","print('All objects observed:\\n{}'.format(objects))\n","print('objectID             {}'.format(objectID))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["rects is:            ((190.98041, 268.44498, 264.6262, 365.7258), (288.77316, 87.79468, 380.10934, 167.98907), (370.46277, 285.23688, 453.4522, 367.9566))\n","Unique is:           [1. 2. 3. 0.]\n","NrObje is:           3\n","Box is:              [370.46277 285.23688 453.4522  367.9566 ]\n","Len(T) is:           3\n","Predicted Boxes are: [370.46277 285.23688 453.4522  367.9566 ]\n","Type of boxes are  : <class 'numpy.ndarray'>\n","Len of unique is:    4\n","All objects observed:\n","OrderedDict([(0, array([227, 317])), (1, array([334, 127])), (2, array([411, 326]))])\n","objectID             2\n"]}]}]}